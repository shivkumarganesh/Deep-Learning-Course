{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Part A Autograd.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMq0dtoWZRScqzz1fCMdNMU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shivkumarganesh/Deep-Learning-Course/blob/main/Assignment-6/Part_A_Autograd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fuc1DTJTKTNm"
      },
      "source": [
        "from scipy.special import softmax\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.utils import np_utils\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HYSDxaQ0lY5"
      },
      "source": [
        "# Creating a Custom Tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiKGOlQdKZ4m"
      },
      "source": [
        "# Reference from the Tutorial Provided\n",
        "class CustomTensor (object):\n",
        "    # Initialization of the CustomTensor (Constructor)\n",
        "    def __init__(self,data,\n",
        "                 autograd=False,\n",
        "                 creators=None,\n",
        "                 creation_op=None,\n",
        "                 id=None):\n",
        "        \n",
        "        self.data = np.array(data)\n",
        "        self.autograd = autograd\n",
        "        self.grad = None\n",
        "        if(id is None):\n",
        "            self.id = np.random.randint(0,100000)\n",
        "        else:\n",
        "            self.id = id\n",
        "        \n",
        "        self.creators = creators\n",
        "        self.creation_op = creation_op\n",
        "        self.children = {}\n",
        "        \n",
        "        if(creators is not None):\n",
        "            for c in creators:\n",
        "                if(self.id not in c.children):\n",
        "                    c.children[self.id] = 1\n",
        "                else:\n",
        "                    c.children[self.id] += 1\n",
        "\n",
        "    def all_children_grads_accounted_for(self):\n",
        "        for id,cnt in self.children.items():\n",
        "            if(cnt != 0):\n",
        "                return False\n",
        "        return True \n",
        "        \n",
        "    def backward(self,grad=None, grad_origin=None):\n",
        "        if(self.autograd):\n",
        " \n",
        "            if(grad is None):\n",
        "                grad = CustomTensor(np.ones_like(self.data))\n",
        "\n",
        "            if(grad_origin is not None):\n",
        "                if(self.children[grad_origin.id] == 0):\n",
        "                    raise Exception(\"cannot backprop more than once\")\n",
        "                else:\n",
        "                    self.children[grad_origin.id] -= 1\n",
        "\n",
        "            if(self.grad is None):\n",
        "                self.grad = grad\n",
        "            else:\n",
        "                self.grad += grad\n",
        "            \n",
        "            # grads must not have grads of their own\n",
        "            assert grad.autograd == False\n",
        "            \n",
        "            # only continue backpropping if there's something to\n",
        "            # backprop into and if all gradients (from children)\n",
        "            # are accounted for override waiting for children if\n",
        "            # \"backprop\" was called on this variable directly\n",
        "            if (self.creators is not None and \n",
        "               (self.all_children_grads_accounted_for() or \n",
        "                grad_origin is None)):\n",
        "\n",
        "                if (self.creation_op == \"add\"):\n",
        "                  self.creators[0].backward(self.grad, self)\n",
        "                  self.creators[1].backward(self.grad, self)\n",
        "                    \n",
        "                if (self.creation_op == \"sub\"):\n",
        "                  self.creators[0].backward(CustomTensor(self.grad.data), self)\n",
        "                  self.creators[1].backward(CustomTensor(self.grad.__neg__().data), self)\n",
        "\n",
        "                if (self.creation_op == \"mul\"):\n",
        "                  new = self.grad * self.creators[1]\n",
        "                  self.creators[0].backward(new , self)\n",
        "                  new = self.grad * self.creators[0]\n",
        "                  self.creators[1].backward(new, self)                    \n",
        "                    \n",
        "                if (self.creation_op == \"mm\"):\n",
        "                  c0 = self.creators[0]\n",
        "                  c1 = self.creators[1]\n",
        "                  new = self.grad.mm(c1.transpose())\n",
        "                  c0.backward(new)\n",
        "                  new = self.grad.transpose().mm(c0).transpose()\n",
        "                  c1.backward(new)\n",
        "                    \n",
        "                if (self.creation_op == \"transpose\"):\n",
        "                  self.creators[0].backward(self.grad.transpose())\n",
        "\n",
        "                if (\"sum\" in self.creation_op):\n",
        "                  dim = int(self.creation_op.split(\"_\")[1])\n",
        "                  self.creators[0].backward(self.grad.expand(dim,self.creators[0].data.shape[dim]))\n",
        "\n",
        "\n",
        "                if (\"expand\" in self.creation_op):\n",
        "                  dim = int(self.creation_op.split(\"_\")[1])\n",
        "                  self.creators[0].backward(self.grad.sum(dim))\n",
        "                \n",
        "\n",
        "                if self.creation_op == \"softmax\":\n",
        "                  self.creators[0].backward(self.grad)\n",
        "                    \n",
        "                if(self.creation_op == \"neg\"):\n",
        "                  self.creators[0].backward(self.grad.__neg__())\n",
        "                    \n",
        "    def __add__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return CustomTensor(self.data + other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"add\")\n",
        "        return CustomTensor(self.data + other.data)\n",
        "\n",
        "    def __neg__(self):\n",
        "        if(self.autograd):\n",
        "            return CustomTensor(self.data * -1,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"neg\")\n",
        "        return CustomTensor(self.data * -1)\n",
        "    \n",
        "    def __sub__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return CustomTensor(self.data - other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"sub\")\n",
        "        return CustomTensor(self.data - other.data)\n",
        "    \n",
        "    def __mul__(self, other):\n",
        "        if(self.autograd and other.autograd):\n",
        "            return CustomTensor(self.data * other.data,\n",
        "                          autograd=True,\n",
        "                          creators=[self,other],\n",
        "                          creation_op=\"mul\")\n",
        "        return CustomTensor(self.data * other.data)    \n",
        "\n",
        "    def sum(self, dim):\n",
        "        if(self.autograd):\n",
        "            return CustomTensor(self.data.sum(dim),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"sum_\"+str(dim))\n",
        "        return CustomTensor(self.data.sum(dim))\n",
        "    \n",
        "    def expand(self, dim,copies):\n",
        "\n",
        "        trans_cmd = list(range(0,len(self.data.shape)))\n",
        "        trans_cmd.insert(dim,len(self.data.shape))\n",
        "        new_data = self.data.repeat(copies).reshape(list(self.data.shape) + [copies]).transpose(trans_cmd)\n",
        "        \n",
        "        if(self.autograd):\n",
        "            return CustomTensor(new_data,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"expand_\"+str(dim))\n",
        "        return CustomTensor(new_data)\n",
        "    \n",
        "    def transpose(self):\n",
        "        if(self.autograd):\n",
        "            return CustomTensor(self.data.transpose(),\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"transpose\")\n",
        "        \n",
        "        return CustomTensor(self.data.transpose())\n",
        "    \n",
        "    def mm(self, x):\n",
        "        if(self.autograd):\n",
        "            return CustomTensor(self.data.dot(x.data),\n",
        "                          autograd=True,\n",
        "                          creators=[self,x],\n",
        "                          creation_op=\"mm\")\n",
        "        return CustomTensor(self.data.dot(x.data))\n",
        "    \n",
        "    def softmax(self):\n",
        "        x = self.data - self.data.max(axis=1, keepdims=True)\n",
        "        y = np.exp(x)\n",
        "        v = y / y.sum(axis=1, keepdims=True)\n",
        "\n",
        "        if self.autograd:\n",
        "            return CustomTensor(v,\n",
        "                          autograd=True,\n",
        "                          creators=[self],\n",
        "                          creation_op=\"softmax\")\n",
        "        return CustomTensor(v)\n",
        "    \n",
        "    def __repr__(self):\n",
        "        return str(self.data.__repr__())\n",
        "    \n",
        "    def __str__(self):\n",
        "        return str(self.data.__str__())  "
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyJMvw7rQz_B"
      },
      "source": [
        "x = CustomTensor(['hello'])\n",
        "x1 = CustomTensor([1,2,3,4,5], autograd=True)\n",
        "x2 = CustomTensor([1.,2.,3.,4.,5.], autograd=True)\n",
        "x3 = CustomTensor([5,4,3,2,1], autograd=True)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQ5rufjQQ3z_",
        "outputId": "03afcff8-7051-4e54-afaa-5891de52b79b"
      },
      "source": [
        "print(x)\n",
        "print(x1)\n",
        "print(x2)\n",
        "print(x3)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['hello']\n",
            "[1 2 3 4 5]\n",
            "[1. 2. 3. 4. 5.]\n",
            "[5 4 3 2 1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4yk-5_60p3z"
      },
      "source": [
        "# Importing MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiLFX3DxQ4ND",
        "outputId": "dad53a39-0964-4f2c-bf10-c59ccc6f360c"
      },
      "source": [
        "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
        "X_train = X_train.reshape(X_train.shape[0], 28*28) / 255.\n",
        "X_test = X_test.reshape(X_test.shape[0], 28*28) / 255.\n",
        "Y_train = np_utils.to_categorical(Y_train)\n",
        "Y_test = np_utils.to_categorical(Y_test)\n",
        "\n",
        "print('X_train.shape', X_train.shape)\n",
        "print('X_test.shape', X_test.shape)\n",
        "print('Y_train.shape', Y_train.shape)\n",
        "print('Y_test.shape', Y_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train.shape (60000, 784)\n",
            "X_test.shape (10000, 784)\n",
            "Y_train.shape (60000, 10)\n",
            "Y_test.shape (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jfhJgG1W0swG"
      },
      "source": [
        "# MNIST Data classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77v2NHBMRYPF",
        "outputId": "4a686f6b-05eb-4f3a-acfb-cd7973a5f806"
      },
      "source": [
        "data = CustomTensor(X_train, autograd=True)\n",
        "target = CustomTensor(Y_train, autograd=True)\n",
        "\n",
        "w = list()\n",
        "w.append(CustomTensor(np.random.rand(X_train.shape[1], 64), autograd=True))\n",
        "w.append(CustomTensor(np.random.rand(64, 10), autograd=True))\n",
        "\n",
        "lr = 1e-3\n",
        "\n",
        "for i in range(100):\n",
        "    # Predict\n",
        "    pred = data.mm(w[0]).mm(w[1]).softmax()\n",
        "\n",
        "    # Compare\n",
        "    loss = ((pred - target) * (pred - target)).sum(0)\n",
        "\n",
        "    # Learn\n",
        "    loss.backward(CustomTensor(np.ones_like(loss.data)))\n",
        "\n",
        "    for w_ in w:\n",
        "        w_.data -= w_.grad.data * lr\n",
        "        w_.grad.data *= 0\n",
        "\n",
        "    print(np.mean(loss.data))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10445.45999677915\n",
            "10651.6\n",
            "10651.6\n",
            "10651.6\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n",
            "10829.8\n",
            "10815.4\n",
            "10815.4\n",
            "10815.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:175: RuntimeWarning: invalid value encountered in subtract\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n",
            "nan\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}